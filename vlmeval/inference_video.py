import torch
import torch.distributed as dist
from vlmeval.config import supported_VLM
from vlmeval.utils import track_progress_rich
from vlmeval.smp import *

from concurrent.futures import ThreadPoolExecutor, as_completed, wait, FIRST_COMPLETED
import time
from queue import Empty, Full

FAIL_MSG = 'Failed to obtain answer via API.'
FAIL_MSGS = [
    'Failed to obtain answer via API.',
    '[ERROR]',
    'Hit max new token.',
    'Failed: Model exited.',
    'Failed',
    '<',
]

logger = get_logger(name='test')

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, nargs='+', required=True)
    parser.add_argument('--model', type=str, nargs='+', required=True)
    parser.add_argument('--nproc', type=int, default=4, required=True)
    parser.add_argument('--verbose', action='store_true')
    args = parser.parse_args()
    return args


# Only API model is accepted
def infer_data_api(model, work_dir, model_name, dataset, samples_dict={}, api_nproc=4):
    rank, world_size = get_rank_and_world_size()
    assert rank == 0 and world_size == 1
    dataset_name = dataset.dataset_name
    model = supported_VLM[model_name]() if isinstance(model, str) else model
    assert getattr(model, 'is_api', False)

    indices = list(samples_dict.keys())
    if getattr(model,'backend', None) == 'genai':
        if dataset.nframe > 0:
            print(
                'Gemini model (with genai backend) does not support nframe, '
                'will set its VIDEO_LLM to False to enable multi-image input for video.'
            )
            setattr(model, 'VIDEO_LLM', False)
        else:
            print('Gemini model (with genai backend) is a video-llm, '
                  'will reset fps setting in model to match the dataset.')
            setattr(model, 'fps', dataset.fps)
            print(f'The fps is set to {dataset.fps} for the model {model_name}.')
    elif getattr(model,'backend', None) == 'vertex':
        print('Gemini model (with vertex backend) does not support video input, '
              'will set its VIDEO_LLM to False to enable multi-image input for video.')
        setattr(model, 'VIDEO_LLM', False)

    packstr = 'pack' if getattr(dataset, 'pack', False) else 'nopack'
    build_prompt_input = [(samples_dict[idx], getattr(model, 'VIDEO_LLM', False)) for idx in indices]
    if dataset.nframe > 0:
        struct_tmp_file = f'{work_dir}/{model_name}_{dataset_name}_{dataset.nframe}frame_{packstr}_structs.pkl'
    else:
        struct_tmp_file = f'{work_dir}/{model_name}_{dataset_name}_{dataset.fps}fps_{packstr}_structs.pkl'
    structs = track_progress_rich(
        dataset.build_prompt,
        tasks=build_prompt_input,
        nproc=api_nproc,
        save=struct_tmp_file,
        keys=indices,
    )

    if dataset.nframe > 0:
        out_file = f'{work_dir}/{model_name}_{dataset_name}_{dataset.nframe}frame_{packstr}_supp.pkl'
    else:
        out_file = f'{work_dir}/{model_name}_{dataset_name}_{dataset.fps}fps_{packstr}_supp.pkl'
    res = load(out_file) if osp.exists(out_file) else {}

    structs = [s for i, s in zip(indices, structs) if i not in res or res[i] == FAIL_MSG]
    structs = [struct for struct in structs if struct is not None]
    indices = [i for i in indices if i not in res or res[i] == FAIL_MSG]

    gen_func = model.generate
    structs = [dict(message=struct, dataset=dataset_name) for struct in structs]

    if len(structs):
        track_progress_rich(gen_func, structs, nproc=api_nproc, chunksize=api_nproc, save=out_file, keys=indices)

    res = load(out_file)
    return res


def infer_data(model, model_name, work_dir, dataset, out_file, verbose=False, api_nproc=4, use_vllm=False):
    res = load(out_file) if osp.exists(out_file) else {}
    rank, world_size = get_rank_and_world_size()
    dataset_name = dataset.dataset_name

    sample_indices = list(dataset.videos) if getattr(dataset, 'pack', False) else list(dataset.data['index'])
    samples = list(dataset.videos) if getattr(dataset, 'pack', False) else list(range(len(dataset.data)))
    sample_map = {i: s for i, s in zip(sample_indices, samples)}

    sample_indices_sub = sample_indices[rank::world_size]
    if np.all([idx in res for idx in sample_indices_sub]):
        return model
    sample_indices_subrem = [x for x in sample_indices_sub if x not in res]

    kwargs = {}
    if model_name is not None and (
        'Llama-4' in model_name
        or 'Qwen2-VL' in model_name
        or 'Qwen2.5-VL' in model_name
        or 'Qwen2.5-Omni' in model_name
    ):
        kwargs = {'use_vllm': use_vllm}

    # (25.06.05) In newer version of transformers (after 4.50), with device_map='auto' and torchrun launcher,
    # Transformers automatically adopt TP parallelism, which leads to compatibility problems with VLMEvalKit
    # (In VLMEvalKit, we use torchrun to launch multiple model instances on a single node).
    # To bypass this problem, we unset `WORLD_SIZE` before building the model to not use TP parallel.
    ws_bak = os.environ.pop('WORLD_SIZE', None)
    model = supported_VLM[model_name](**kwargs) if isinstance(model, str) else model
    if ws_bak:
        os.environ['WORLD_SIZE'] = ws_bak

    is_api = getattr(model, 'is_api', False)
    if is_api:
        assert world_size == 1
        supp = infer_data_api(
            model=model,
            work_dir=work_dir,
            model_name=model_name,
            dataset=dataset,
            samples_dict={k: sample_map[k] for k in sample_indices_subrem},
            api_nproc=api_nproc)
        for k in sample_indices_subrem:
            assert k in supp
        res.update(supp)
        dump(res, out_file)
        return model

    assert not getattr(dataset, 'pack', False), 'Current model not supported pack mode!'
    if 'megabench' in dataset_name.lower() and 'llava_onevision' in model_name:
        print(
            'LLaVA-OneVision does not support Megabench dataset as video dataset, '
            'will set its VIDEO_LLM to False to enable multi-image input for video.'
        )
        setattr(model, 'VIDEO_LLM', False)

    for i, idx in tqdm(enumerate(sample_indices_subrem)):
        if idx in res:
            continue
        if getattr(model, 'nframe', None) is not None and getattr(model, 'nframe', 0) > 0:
            if dataset.nframe > 0:
                if getattr(model, 'nframe', 0) != dataset.nframe:
                    print(f'{model_name} is a video-llm model, nframe is set to {dataset.nframe}, not using default')
                    setattr(model, 'nframe', dataset.nframe)
            elif getattr(model, 'fps', 0) == 0:
                raise ValueError(f'fps is not suitable for {model_name}')
            else:
                setattr(model, 'nframe', None)
        if getattr(model, 'fps', None) is not None and getattr(model, 'fps', 0) > 0:
            if dataset.fps > 0:
                if getattr(model, 'fps', 0) != dataset.fps:
                    print(f'{model_name} is a video-llm model, fps is set to {dataset.fps}, not using default')
                    setattr(model, 'fps', dataset.fps)
            elif getattr(model, 'nframe', 0) == 0:
                raise ValueError(f'nframe is not suitable for {model_name}')
            else:
                setattr(model, 'fps', None)
        if (
            'Qwen2-VL' in model_name
            or 'Qwen2.5-VL' in model_name
            or 'Qwen2.5-Omni' in model_name
        ):
            if getattr(model, 'nframe', None) is None and dataset.nframe > 0:
                print(f'using {model_name} default setting for video, dataset.nframe is ommitted')
            if getattr(model, 'fps', None) is None and dataset.fps > 0:
                print(f'using {model_name} default setting for video, dataset.fps is ommitted')
        if 'SUB_DATASET' in dataset.data.iloc[sample_map[idx]]:
            dataset_name = dataset.data.iloc[sample_map[idx]]['SUB_DATASET']
        if hasattr(model, 'use_custom_prompt') and model.use_custom_prompt(dataset_name):
            if dataset.nframe == 0:
                raise ValueError(f'nframe must be set for custom prompt, fps is not suitable for {model_name}')
            struct = model.build_prompt(
                dataset.data.iloc[sample_map[idx]], dataset=dataset, video_llm=getattr(model, 'VIDEO_LLM', False)
            )
        else:
            struct = dataset.build_prompt(
                sample_map[idx], video_llm=getattr(model, 'VIDEO_LLM', False)
            )
        if struct is None:
            continue

        # If `SKIP_ERR` flag is set, the model will skip the generation if error is encountered
        if os.environ.get('SKIP_ERR', False) == '1':
            FAIL_MSG = 'Failed to obtain answer'
            try:
                response = model.generate(message=struct, dataset=dataset_name)
            except RuntimeError as err:
                torch.cuda.synchronize()
                warnings.error(f'{type(err)} {str(err)}')
                response = f'{FAIL_MSG}: {type(err)} {str(err)}'
        else:
            response = model.generate(message=struct, dataset=dataset_name)
        torch.cuda.empty_cache()

        if verbose:
            print(response, flush=True)

        res[idx] = response
        if (i + 1) % 20 == 0:
            dump(res, out_file)

    res = {k: res[k] for k in sample_indices_sub}
    dump(res, out_file)
    return model


# A wrapper for infer_data, do the pre & post processing
def infer_data_job_video(
        model,
        work_dir,
        model_name,
        dataset,
        result_file_name,
        verbose=False,
        api_nproc=4,
        use_vllm=False):

    dataset_name = dataset.dataset_name
    rank, world_size = get_rank_and_world_size()
    result_file = osp.join(work_dir, result_file_name)
    # Dump Predictions to Prev File if result file exists
    # if osp.exists(result_file):
    #     return model

    tmpl = osp.join(work_dir, '{}' + f'{world_size}_{osp.splitext(result_file_name)[0]}.pkl')
    out_file = tmpl.format(rank)

    print(f"outfile: {out_file}")

    if osp.exists(result_file):

        print(f"Result file exist: {result_file}, dump to pkl.")

        res = load(result_file)

        results = {k: v for k, v in zip(res['index'], res['prediction'])}
        results = {k: v for k, v in results.items()
                if not any(msg in str(v) for msg in FAIL_MSGS)}

        dump(results, out_file)


    if 'ddp' in model_name.lower() or 'single' in model_name.lower():
        devices = list(range(torch.cuda.device_count()))
        replicas_per_device = int(os.environ.get('replicas_per_device', 1))                # æˆ– 2

        print(f"in ddp --------------------------")

        model = infer_data_unified(
            model=model,                  # å¯ä»¥æ˜¯æ³¨å†Œåå­—ç¬¦ä¸²ï¼Œæˆ–å·²æ„é€ å¥½çš„å®ä¾‹
            model_name=model_name,
            work_dir=work_dir,
            dataset=dataset,
            out_file=out_file,
            verbose=verbose,
            api_nproc=api_nproc,
            use_vllm=use_vllm,
            devices=devices,              # ä¾‹å¦‚ [0,1,2,3]ï¼›ä¸ä¼ åˆ™é»˜è®¤ç”¨æ‰€æœ‰å¯è§ GPU
            replicas_per_device=replicas_per_device  # ä¾‹å¦‚ 2ï¼ˆæ¯å¡èµ·2ä¸ªè¿›ç¨‹/æ¨¡å‹ï¼‰
        )
    else:
        model = infer_data(
            model=model,
            model_name=model_name,
            work_dir=work_dir,
            dataset=dataset,
            out_file=out_file,
            verbose=verbose,
            api_nproc=api_nproc,
            use_vllm=use_vllm)

    if world_size > 1:
        dist.barrier()

    if rank == 0:
        data_all = {}
        for i in range(world_size):
            data_all.update(load(tmpl.format(i)))

        meta = dataset.data
        if dataset_name == 'MMBench-Video' and getattr(dataset, 'pack', False):
            meta, vstats = dataset.load_pack_answers(data_all)
            print(f'Statitics of Pack Video Inference: {vstats}')
        else:
            for x in meta['index']:
                assert x in data_all
            meta['prediction'] = [str(data_all[x]) for x in meta['index']]
            if 'image' in meta:
                meta.pop('image')

        dump(meta, result_file)
        for i in range(world_size):
            os.remove(tmpl.format(i))
    return model


def make_registry_builder(model_name: str, extra_kwargs: dict):
    ctor = supported_VLM[model_name]  # å¯èƒ½æ˜¯ç±»æˆ– partialï¼Œåæ­£æ˜¯å¯è°ƒç”¨
    def _builder(*, device_id=None, force_single_device=True, **kw):
        # ä¼ å‚ä¼˜å…ˆçº§ï¼šè°ƒç”¨ä¾§ kw > extra_kwargs
        merged = {**extra_kwargs, **kw}
        return ctor(device_id=device_id, force_single_device=force_single_device, **merged)
    return _builder


# ========= å­è¿›ç¨‹ï¼šä¸€è¿›ç¨‹ç»‘å®šä¸€å¼ ç‰©ç†å¡ï¼›è¿›ç¨‹å†…åªè§åˆ°â€œå±€éƒ¨ cuda:0â€ =========
def _video_worker_loop(physical_gpu_id, in_q, out_q, model_name: str, extra_kwargs: dict):
    import os, sys, warnings
    # 1) å…ˆéš”ç¦»å¯è§å¡ï¼ˆåŠ¡å¿…åœ¨ import torch/transformers ä¹‹å‰ï¼‰
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    # os.environ["CUDA_VISIBLE_DEVICES"] = str(physical_gpu_id)
    for k in ("RANK", "LOCAL_RANK", "WORLD_SIZE"):
        os.environ.pop(k, None)

    # 2) è®©å­è¿›ç¨‹èƒ½ import åˆ°ä½ çš„å·¥ç¨‹åŒ…ï¼ˆæŒ‰ä½ çš„è·¯å¾„æ”¹ï¼‰
    pkg_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if pkg_root not in sys.path:
        sys.path.insert(0, pkg_root)

    # 3) å† importï¼ˆæ­¤æ—¶åªä¼šçœ‹åˆ° 1 å¼ å¡ï¼šlocal cuda:0ï¼‰
    import torch

    # 4) æ„é€ æ¨¡å‹ â€”â€” è¿›ç¨‹å†…åªè§åˆ°ä¸€å¼ å¡ï¼Œç»‘å®šâ€œå±€éƒ¨ cuda:0â€
    kw = dict(extra_kwargs or {})
    kw["force_single_device"] = True
    kw["device_id"] = physical_gpu_id  # è¿›ç¨‹å†…å±€éƒ¨ 0
    builder = make_registry_builder(model_name, {})
    model = builder(**kw)

    model.fps = kw['fps']
    model.nframe = kw['nframe']

    # 5) å¤„ç†ä»»åŠ¡
    while True:
        item = in_q.get()
        if item is None:
            break
        idx, struct, dataset_name = item
        try:
            resp = model.generate(message=struct, dataset=dataset_name)
        except RuntimeError as err:
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            warnings.warn(f'{type(err)} {str(err)}')
            resp = f'Failed to obtain answer: {type(err)} {str(err)}'
        except Exception as err:
            resp = f'Failed to obtain answer: {type(err)} {str(err)}'
        out_q.put((idx, resp))

        logger.info(f"queue put idx : {idx} {resp}")

def infer_data_unified(model,
               model_name,
               work_dir,
               dataset,
               out_file,
               verbose=True,
               api_nproc=4,
               use_vllm=False,
               devices=None,
               replicas_per_device=1):
    """
    å¤šå¡å¤šæ¨¡å‹ï¼ˆå¤šè¿›ç¨‹ï¼‰è§†é¢‘æ¨ç†ç‰ˆæœ¬ï¼š
    - å½“ `model` æ˜¯å­—ç¬¦ä¸²ï¼ˆæ³¨å†Œåï¼‰æ—¶ï¼šçˆ¶è¿›ç¨‹èµ·å­è¿›ç¨‹æ± ï¼ˆæ¯å¡ replicas_per_device ä¸ªï¼‰ï¼Œ
      çˆ¶è¿›ç¨‹æ„å»ºè¾“å…¥å¹¶è½®è¯¢åˆ†å‘åˆ°å­è¿›ç¨‹ï¼›å­è¿›ç¨‹åœ¨æœ¬åœ° cuda:0 ä¸Šæ¨ç†å¹¶å›ä¼ ç»“æœã€‚
    - å½“ `model` å·²æ˜¯å®ä¾‹æ—¶ï¼šä¿æŒä½ çš„åŸå§‹ä¸²è¡Œè·¯å¾„ã€‚
    """
    import os, warnings, numpy as np, torch, multiprocessing as mp
    from tqdm import tqdm

    res = load(out_file) if osp.exists(out_file) else {}
    rank, world_size = get_rank_and_world_size()
    dataset_name = dataset.dataset_name

    sample_indices = list(dataset.videos) if getattr(dataset, 'pack', False) else list(dataset.data['index'])
    samples = list(dataset.videos) if getattr(dataset, 'pack', False) else list(range(len(dataset.data)))
    sample_map = {i: s for i, s in zip(sample_indices, samples)}

    sample_indices_sub = sample_indices[rank::world_size]
    if np.all([idx in res for idx in sample_indices_sub]):
        return model
    sample_indices_subrem = [x for x in sample_indices_sub if x not in res]
    if len(sample_indices_subrem) == 0:
        return model

    # é¢å¤– kwargs
    kwargs = {}
    if model_name is not None and (
        'Llama-4' in model_name
        or 'Qwen2-VL' in model_name
        or 'Qwen2.5-VL' in model_name
        or 'Qwen2.5-Omni' in model_name
    ):
        kwargs = {'use_vllm': use_vllm}

    # ç»Ÿä¸€ï¼šåªåœ¨â€œæ¨¡å‹å·²å®ä¾‹â€æ—¶æœ¬åœ°æ„é€ ï¼›å­—ç¬¦ä¸²æ—¶ç”±å­è¿›ç¨‹æ„é€ 
    if not isinstance(model, str):
        # ====== åŸä¸²è¡Œè·¯å¾„ï¼ˆä¿æŒä½ ç°æœ‰é€»è¾‘ä¸å˜ï¼‰ ======
        # Qwen é»˜è®¤ VIDEO_LLM æç¤º
        if 'megabench' in dataset_name.lower() and 'llava_onevision' in model_name:
            print(
                'LLaVA-OneVision does not support Megabench dataset as video dataset, '
                'will set its VIDEO_LLM to False to enable multi-image input for video.'
            )
            setattr(model, 'VIDEO_LLM', False)

        for i, idx in tqdm(enumerate(sample_indices_subrem), total=len(sample_indices_subrem),
                           desc=f'Infer {model_name}/{dataset_name}, Rank {rank}/{world_size}'):
            if idx in res:
                continue

            # â€”â€” ä¸ä½ åŸæœ¬å®Œå…¨ä¸€è‡´çš„ â€œè§†é¢‘å‚æ•°å¯¹é½ + æ„é€ è¾“å…¥â€ â€”â€” #
            if getattr(model, 'nframe', None) is not None and getattr(model, 'nframe', 0) > 0:
                if dataset.nframe > 0:
                    if getattr(model, 'nframe', 0) != dataset.nframe:
                        print(f'{model_name} is a video-llm model, nframe is set to {dataset.nframe}, not using default')
                        setattr(model, 'nframe', dataset.nframe)
                elif getattr(model, 'fps', 0) == 0:
                    raise ValueError(f'fps is not suitable for {model_name}')
                else:
                    setattr(model, 'nframe', None)

            if getattr(model, 'fps', None) is not None and getattr(model, 'fps', 0) > 0:
                if dataset.fps > 0:
                    if getattr(model, 'fps', 0) != dataset.fps:
                        print(f'{model_name} is a video-llm model, fps is set to {dataset.fps}, not using default')
                        setattr(model, 'fps', dataset.fps)
                elif getattr(model, 'nframe', 0) == 0:
                    raise ValueError(f'nframe is not suitable for {model_name}')
                else:
                    setattr(model, 'fps', None)

            if (
                'Qwen2-VL' in model_name
                or 'Qwen2.5-VL' in model_name
                or 'Qwen2.5-Omni' in model_name
            ):
                if getattr(model, 'nframe', None) is None and dataset.nframe > 0:
                    print(f'using {model_name} default setting for video, dataset.nframe is ommitted')
                if getattr(model, 'fps', None) is None and dataset.fps > 0:
                    print(f'using {model_name} default setting for video, dataset.fps is ommitted')

            dname = dataset_name
            if 'SUB_DATASET' in dataset.data.iloc[sample_map[idx]]:
                dname = dataset.data.iloc[sample_map[idx]]['SUB_DATASET']

            if hasattr(model, 'use_custom_prompt') and model.use_custom_prompt(dname):
                if dataset.nframe == 0:
                    raise ValueError(f'nframe must be set for custom prompt, fps is not suitable for {model_name}')
                struct = model.build_prompt(
                    dataset.data.iloc[sample_map[idx]], dataset=dataset, video_llm=getattr(model, 'VIDEO_LLM', False)
                )
            else:
                struct = dataset.build_prompt(
                    sample_map[idx], video_llm=getattr(model, 'VIDEO_LLM', False)
                )

            # SKIP_ERR ä¿æŒä¸å˜
            if os.environ.get('SKIP_ERR', False) == '1':
                FAIL_MSG = 'Failed to obtain answer'
                try:
                    response = model.generate(message=struct, dataset=dname)
                except RuntimeError as err:
                    torch.cuda.synchronize()
                    warnings.warn(f'{type(err)} {str(err)}')
                    response = f'{FAIL_MSG}: {type(err)} {str(err)}'
            else:
                response = model.generate(message=struct, dataset=dname)
            torch.cuda.empty_cache()

            if verbose:
                print(response, flush=True)
            res[idx] = response
            if ((i + 1) % 20) == 0:
                dump(res, out_file)

        res = {k: res[k] for k in sample_indices_sub}
        dump(res, out_file)
        return model

    # ====== å¤šè¿›ç¨‹å¤šå¡è·¯å¾„ï¼ˆmodel æ˜¯å­—ç¬¦ä¸²ï¼‰ ======
    # è§„é¿ TPï¼šunset WORLD_SIZEï¼Œåªåœ¨å­è¿›ç¨‹æ„é€ æ¨¡å‹ï¼ˆä½ å›¾ç‰‡ç‰ˆä¹Ÿè¿™æ ·åšçš„ï¼‰
    ws_bak = os.environ.pop('WORLD_SIZE', None)

    # è®¾å¤‡åˆ—è¡¨ï¼ˆçˆ¶è¿›ç¨‹è§†è§’çš„â€œç‰©ç† idâ€ï¼‰
    if devices is None:
        devices = list(range(torch.cuda.device_count()))
    assert len(devices) >= 1, "No CUDA devices found."

    # æ”¯æŒæ¯å¡å¤šå‰¯æœ¬ï¼šå±•å¼€ç‰©ç† id åˆ—è¡¨
    physical_device_list = []
    for d in devices:
        physical_device_list.extend([d] * max(1, int(replicas_per_device)))

    # è¿›ç¨‹æ± ä¸é˜Ÿåˆ—
    ctx = mp.get_context("spawn")
    inqs, procs = [], []
    outq = ctx.Queue(maxsize=6000)
    # outq = ctx.SimpleQueue()

    fps = None
    nframe = None

    if dataset.fps > 0:
        fps = dataset.fps
    elif dataset.nframe > 0:
        nframe = dataset.nframe

    kwargs['fps'] = fps
    kwargs['nframe'] = nframe

    # å¯åŠ¨å­è¿›ç¨‹
    for phys_id in physical_device_list:
        iq = ctx.Queue(maxsize=6000)
        # iq = ctx.SimpleQueue()
        p = ctx.Process(
            target=_video_worker_loop,
            args=(phys_id, iq, outq, model_name, kwargs),
            daemon=True
        )
        p.start()
        inqs.append(iq)
        procs.append(p)

    # åˆ†å‘ä»»åŠ¡ï¼ˆçˆ¶è¿›ç¨‹æ„é€ è¾“å…¥ï¼Œä¸¥æ ¼ä¿ç•™ idxï¼‰
    rr = 0
    want = 0
    dispatched = []

    qwen_video_llm_model = [
        # "Qwen2.5-VL-3B-Instruct_DDP",
        # "Qwen2.5-VL-7B-Instruct_DDP",

        # "Qwen3-VL-2B-Instruct_DDP",
        # "Qwen3-VL-4B-Instruct_DDP",
        # "Qwen3-VL-8B-Instruct_DDP",

        "SpaceR-SFT-7B_qwen25_DDP",

        "VST-3B-SFT_DDP",
        "VST-7B-SFT_DDP",

        # "Qwen2.5-VL-3B-Instruct",
        # "Qwen2.5-VL-7B-Instruct",

        # "Qwen3-VL-2B-Instruct",
        # "Qwen3-VL-4B-Instruct",
        # "Qwen3-VL-8B-Instruct",

    ]


    for i, idx in tqdm(enumerate(sample_indices_subrem), total=len(sample_indices_subrem),
                       desc=f'Infer {model_name}/{dataset_name}, Rank {rank}/{world_size}'):
        if idx in res:
            continue

        # â€”â€” ä¸ä¸²è¡Œè·¯å¾„ç›¸åŒçš„â€œè§†é¢‘å‚æ•°å¯¹é½â€ â€”â€” #
        # æ³¨æ„ï¼šè¿™äº›å‚æ•°ä½œç”¨åœ¨â€œçˆ¶è¿›ç¨‹çš„ modelâ€ä¸Šï¼Œä½†æˆ‘ä»¬æ˜¯å¤šè¿›ç¨‹æ„é€ æ¨¡å‹ï¼›
        # å¯¹äºå¤šæ•°æ¨¡å‹ï¼Œnframe/fps æ˜¯â€œæ„é€ è¾“å…¥â€å±‚é¢çš„è¦æ±‚ï¼Œä»ä»¥æ„é€  struct ä¸ºå‡†ã€‚
        dname = dataset_name
        if 'SUB_DATASET' in dataset.data.iloc[sample_map[idx]]:
            dname = dataset.data.iloc[sample_map[idx]]['SUB_DATASET']

        if hasattr(model_name, 'use_custom_prompt') and False:
            # model_name æ˜¯å­—ç¬¦ä¸²ï¼Œä¸èƒ½è°ƒå®ä¾‹æ–¹æ³•ï¼›ä¿æŒ dataset ä¾§ç»Ÿä¸€æ„é€ 
            pass

        # è¿™é‡Œç›´æ¥ç”¨ dataset ä¾§æ„é€ ï¼ˆå’Œä½ åŸé€»è¾‘ä¸€è‡´ï¼‰
        video_llm = False if ('megabench' in dataset_name.lower() and 'llava_onevision' in model_name) else getattr(model, 'VIDEO_LLM', False)

        video_llm = True if model_name in qwen_video_llm_model else video_llm
        # video_llm = False

        # print(f"video llm : {video_llm}, model_name.lower(): {dataset_name.lower()}")

        struct = dataset.build_prompt(
            sample_map[idx], video_llm=video_llm
        )

        # inqs[rr % len(inqs)].put((idx, struct, dname))

        q = inqs[rr % len(inqs)]
        while True:
            try:
                q.put((idx, struct, dname), timeout=1.0)
                break
            except Full:
                # å¿ƒè·³ï¼šè‹¥æ‰€æœ‰è¿›ç¨‹éƒ½ä¸åœ¨äº†ï¼Œå°±æŠ¥é”™é€€å‡º
                if sum(p.is_alive() for p in procs) == 0:
                    raise RuntimeError("All video workers exited while dispatching tasks")

        rr += 1
        want += 1
        dispatched.append(idx)

    # å…³é—­è¾“å…¥
    for iq in inqs:
        iq.put(None)

    # æ”¶é›† + æ¯ 20 æ¡è½ç›˜
    completed = 0

    import queue as _q
    import time

    time.sleep(60)

    while completed < want:
        try:
            idx, response = outq.get(timeout=30.0)
        except Empty:
            alive = sum(p.is_alive() for p in procs)
            logger.info(f"[collector] waiting... completed={completed}/{want}, alive={alive}")
            if alive == 0:
                logger.warning(f"All video workers exited early: completed={completed}, want={want}, remain save as Failed.")

                pending = [i for i in dispatched if i not in res]

                if pending:
                    for i in pending:
                        # è‹¥ä½ çš„ response éœ€è¦æºå¸¦æ›´å¤šä¸Šä¸‹æ–‡ï¼Œå¯åœ¨æ­¤æ„é€ 
                        res[i] = FAIL_MSGS[3]
                    dump(res, out_file)
                    completed += len(pending)
                    break

            continue
        res[idx] = response
        # if verbose:
        logger.info(f"queue get idx: {idx} {response}")

        completed += 1
        if (completed % 20) == 0:
            dump(res, out_file)
            logger.info(f"ğŸ“€ checkpoint saved at {completed}/{want}")

    # æœ€ç»ˆè½ç›˜ & æ¢å¤ WORLD_SIZE
    dump(res, out_file)
    if ws_bak:
        os.environ['WORLD_SIZE'] = ws_bak

    # å›æ”¶å­è¿›ç¨‹
    for p in procs:
        p.join(timeout=0.2)

    return model
